{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4993da90",
   "metadata": {},
   "source": [
    "# Parent vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf56f04",
   "metadata": {},
   "source": [
    "When splitting documents for retrieval, there are often conflicting desires:\n",
    "\n",
    "1. You may want to have small documents, so that their embeddings can most accurately reflect their meaning. If too long, then the embeddings can lose meaning.\n",
    "2. You want to have long enough documents that the context of each chunk is retained.\n",
    "\n",
    "The `ParentDocumentVectorStore` strikes that balance by splitting and storing small chunks of data. During retrieval, it first fetches the small chunks but then looks up the parent ids for those chunks and returns those larger documents.\n",
    "\n",
    "The challenge is to manage the life cycle of the three levels of documents correctly:\n",
    "- original documents\n",
    "- chunks extracted from the original documents\n",
    "- transformations of chunks in order to have more vectors with which to retrieve them\n",
    "\n",
    "The `ParentDocumentVectorStore`, in combinaison with others components, is here for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730b0fd3-82d3-439a-87b8-3ef77d9d1d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install 'langchain-parent' openai tiktokena\n",
    "!poetry install -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2631007b-61df-4a1d-93d8-e1ae6d85193e",
   "metadata": {},
   "source": [
    "For the sample, we use the set of documents from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd8e4f1-7c3f-492b-b88f-fb2c9fb82d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_results = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01637476-49ee-4da2-839d-fee62c934f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"names the major mathematical disciplines\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6a3c4bb3423122",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade pip langchain wikipedia > /dev/null\n",
    "from langchain.retrievers import WikipediaRetriever\n",
    "\n",
    "documents = WikipediaRetriever(top_k_results=top_k_results).get_relevant_documents(\"mathematic\")\n",
    "len(documents), query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1642f89f-15f9-4a65-86ad-9c8403393d14",
   "metadata": {},
   "source": [
    "# Select provider\n",
    "## Select the LLM\n",
    "Before starting, you need to:\n",
    "- Set the environment variables\n",
    "- choose an LLM, get the context size, and set the max_tokens to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8340e2-1b4a-4b93-b407-268f8a698496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"XXXXX\"\n",
    "if \"COHERE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"COHERE_API_KEY\"] = \"XXXX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee04f135-a013-47e4-9d18-2fe35896787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet openai tiktoken\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "context_size = 512  # For the demonstration use a smal context_size.\n",
    "max_tokens = int(context_size * (10 / 100))  # 10% for the response\n",
    "max_input_tokens = context_size - max_tokens\n",
    "llm = OpenAI(\n",
    "    max_tokens=1000,\n",
    ")\n",
    "context_size, max_tokens, max_input_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd2a2d9-de21-4336-9711-413f1e9c83b4",
   "metadata": {},
   "source": [
    "## Select the embedding implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ce9c7-fafc-493e-b5b4-93f64f81a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cc5d45-573b-4540-8738-16115818a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a cache\n",
    "import tempfile\n",
    "\n",
    "CACHE_EMBEDDING_PATH = tempfile._gettempdir() + \"/cache_embedding\"\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "fs = LocalFileStore(CACHE_EMBEDDING_PATH)\n",
    "\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "\n",
    "embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, fs, namespace=embeddings.model if hasattr(embeddings, \"model\") else \"unknown\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ff47d-933e-47be-9849-bccc78cf9309",
   "metadata": {},
   "source": [
    "# Transform documents\n",
    "The idea is to transform a document into several versions, and calculate the vector for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b6e300-50f4-41ae-a524-05030c2c865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import *\n",
    "from langchain_parent.document_transformers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e513cf-6ac8-4121-81b9-b961afd52d6f",
   "metadata": {},
   "source": [
    "The first step is to split the document to be compatible with the `max_input_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7a03e9-993c-4208-bfc2-b27199214fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_transformer = TokenTextSplitter(\n",
    "    chunk_size=max_input_tokens, chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a640d55a-a466-4de1-abfe-4d5c1d2d76a3",
   "metadata": {},
   "source": [
    "Apply the transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb10dc0-6cf2-4e14-b55c-bb846ba97515",
   "metadata": {},
   "outputs": [],
   "source": [
    "child_documents = parent_transformer.transform_documents(documents)\n",
    "f\"before:{len(documents)} (big documents), after:{len(child_documents)} (chunk of documents)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aa4959-1325-43cd-a22e-0f7c983c8be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "child_transformer = DocumentTransformerPipeline(\n",
    "    transformers=[\n",
    "        CopyDocumentTransformer(),\n",
    "        GenerateQuestions.from_llm(llm),\n",
    "        SummarizeTransformer.from_llm(llm),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74358ce-b013-4729-a71b-ee4c82f0d51f",
   "metadata": {},
   "source": [
    "Note, we need all the transformation for each chunk, and we also want the original chunk. It's why we add the `CopyDocumentTransformer()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec831c0-f2a9-4735-a5e5-28b0bb312d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "variations_of_chunks = child_transformer.transform_documents(child_documents)\n",
    "variations_of_chunks[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a594c2a-1ddc-4eb7-a7ba-922923fe5dae",
   "metadata": {},
   "source": [
    "# Save all variations in a vector store\n",
    "Now we want to save the chunks and their variations in a vector store. During retrieval, it first fetches the small chunks but then looks up the parent ids for those chunks and returns those larger documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f3d840-a139-4c3e-9d6f-c5bc794c3508",
   "metadata": {},
   "source": [
    "A specialized vectorstore is here for that: `ParentVectorStore`.\n",
    "It's not a real vectorstore, but a wrapper to another vectorstore. When you add a document, the document is transform with the parent_transformer, and each chunk is enriched with different versions, via the `child_transformer`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b14418-9de3-45b0-bb1f-d88f65d21606",
   "metadata": {},
   "source": [
    "We must first, create some persistent component:\n",
    "- A classical vectorstore\n",
    "- A doc store to save each original chunk returned by the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc155113-dd16-4c73-a13c-80d428bd7923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "chroma_vectorstore = Chroma(\n",
    "    collection_name=\"all_variations_of_chunks\",\n",
    "    embedding_function=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d189e7e-3e59-43f0-9a8c-92fd7844c393",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCSTORE_PATH = tempfile._gettempdir() + \"/chunks\"\n",
    "from langchain.storage import EncoderBackedStore\n",
    "from langchain.storage import LocalFileStore\n",
    "import pickle\n",
    "\n",
    "docstore = EncoderBackedStore[str, Document](\n",
    "    store=LocalFileStore(root_path=DOCSTORE_PATH),\n",
    "    key_encoder=lambda x: x,\n",
    "    value_serializer=pickle.dumps,\n",
    "    value_deserializer=pickle.loads\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266663f2-bb6a-46dd-84cc-0ec67a5bf24e",
   "metadata": {},
   "source": [
    "Then, you  must select a metadata to identify each fragment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14003e5f-daab-4dcc-8eef-fd2dcf5fb976",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_key = 'id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fafbad-c577-40ae-8543-a145d9ca7f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_parent.vectorstores import ParentVectorStore\n",
    "\n",
    "vectorstore = ParentVectorStore(\n",
    "    vectorstore=chroma_vectorstore,\n",
    "    docstore=docstore,\n",
    "    parent_id_key=\"source\",\n",
    "    id_key=id_key,\n",
    "    parent_transformer=parent_transformer,\n",
    "    child_transformer=child_transformer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2039782f-9f63-4885-be09-ba9f7555b2cb",
   "metadata": {},
   "source": [
    "Now, it's time to add documents to this vectorstore. \n",
    "- If the `parent_transformer` is set, the document is transformed to a new list of chunks documents (generally it's a split phase).\n",
    "- Then, each chunks documents is transformed with the `child_transformer`.\n",
    "- Each transformation of all chunks is added in the destination vector store (chroma is this sample)\n",
    "- All chunks are saved in the doc store with the list of all associated transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd8eba9-36aa-4ed6-b32d-a7eef48289a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = vectorstore.add_documents(documents)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c5c65eac9a6017",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The ids is the list of id for each chunks. It's possible to use it to delete some chunks. All variations are also deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a7202b-6098-4f41-a570-8909149e5a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.delete(ids[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8c6aeb5cec6e67",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "When you look at the API, you wonder where to save the document IDs from the vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ba9eb40d3768f7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Index vectorstore\n",
    "To manage the live cycle of the documents in the vectorstore, you can use an `index()`.\n",
    "It's a *record manager* to detect when the data are inserted, updated or if a record must be deleted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3ea4486d47ea54",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain.indexes import index, SQLRecordManager\n",
    "\n",
    "record_manager = SQLRecordManager(\n",
    "    namespace=\"record_manager_cache\",\n",
    "    db_url=f\"sqlite:///{tempfile._gettempdir()}/record_manager_cache.db\"\n",
    ")\n",
    "record_manager.create_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29511638aa0cf089",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import doduments via all the pipeline:\n",
    "# - record manager\n",
    "# - docstore\n",
    "# - vectorstore\n",
    "index(\n",
    "    docs_source=documents,  # PPR: on peut y placer un loader\n",
    "    record_manager=record_manager,\n",
    "    vector_store=vectorstore,\n",
    "    cleanup=\"incremental\",\n",
    "    source_id_key=\"source\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec51edafcdd6d2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It's important to know that there are three ways of saving part of the data:\n",
    "- In the *vectorstore*, the bucket, the metadata and the associated embeddings vector\n",
    "- In the *docstore*, the orinal bucket, before the *child_transformations*\n",
    "- In the *SQLRecordManager*, the references of the chunks (FIXME ou doc d'origine ?)\n",
    "Each source does not manage transactions. If a problem occurs while adding a document, it is highly likely that the sources will be inconsistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2981b01e756e9096",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Use a retriever\n",
    "Like with the standard vector store, it's possible to convert to a `VectorStoreRetriever`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec70b4b3dcc80eb8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75188b0269b84da5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selected_chunks = retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca652e175cb0f6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(selected_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d79dba17a046faa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Specialize retrievers\n",
    "It's possible to refine the retrievers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be992243890cc9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "if True:\n",
    "    retriever= MultiQueryRetriever.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8367e9c29f0f542",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # TODO: SelfQuery\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e231bfd97367dc51",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Use a compressor\n",
    "It's possible to use a *compressor*, to filter the selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfc7ec65f722b9e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --quiet cohere\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "cohere_rerank=CohereRerank(\n",
    "    top_n=top_k_results\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cbede89e25ee4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import *\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "similarity_filter = EmbeddingsFilter(\n",
    "        embeddings=embeddings,\n",
    "        similarity_threshold=0.8  # Threshold for determining when two documents are redundant.\n",
    "    )\n",
    "\n",
    "# Filter that drops documents that aren't relevant to the query.\n",
    "drop_filter = LLMChainFilter.from_llm(llm)\n",
    "\n",
    "\n",
    "compressor = DocumentCompressorPipeline(\n",
    "    transformers=[\n",
    "        similarity_filter,\n",
    "        cohere_rerank,\n",
    "    ]\n",
    ")\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_documents = compression_retriever.get_relevant_documents(query)\n",
    "len(selected_chunks),len(compressed_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e195db90457352",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Ask a question\n",
    "Now, it's possible to use this architecture to ask the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f6b9dbd015dab",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever)\n",
    "qa_chain(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae103224-11fd-4955-bb77-b808332539f6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Short version\n",
    "Now it's time to simplify the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5912e70d-019c-4b62-a81d-ee60d3e5fa40",
   "metadata": {},
   "source": [
    "## Prepare the import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a20db69d-d26f-4bb7-b272-d03f780abfb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T07:02:58.396707581Z",
     "start_time": "2023-10-23T07:02:58.345558599Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import EncoderBackedStore\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain_parent.vectorstores import ParentVectorStore\n",
    "from langchain.indexes import index, SQLRecordManager\n",
    "from langchain.text_splitter import *\n",
    "from langchain_parent.document_transformers import *\n",
    "\n",
    "import pickle\n",
    "import tempfile\n",
    "\n",
    "id_key = 'id'\n",
    "top_k_results = 4\n",
    "\n",
    "DOCSTORE_PATH = tempfile._gettempdir() + \"/chunks\"\n",
    "\n",
    "docstore = EncoderBackedStore[str, Document](\n",
    "    store=LocalFileStore(root_path=DOCSTORE_PATH),\n",
    "    key_encoder=lambda x: x,\n",
    "    value_serializer=pickle.dumps,\n",
    "    value_deserializer=pickle.loads\n",
    ")\n",
    "\n",
    "record_manager = SQLRecordManager(\n",
    "    namespace=\"record_manager_cache\",\n",
    "    db_url=f\"sqlite:///{tempfile._gettempdir()}/record_manager_cache.db\"\n",
    ")\n",
    "record_manager.create_schema()\n",
    "\n",
    "parent_transformer = TokenTextSplitter(\n",
    "    chunk_size=max_input_tokens, chunk_overlap=0\n",
    ")\n",
    "child_transformer = DocumentTransformerPipeline(\n",
    "    transformers=[\n",
    "        CopyDocumentTransformer(),\n",
    "        GenerateQuestions.from_llm(llm),\n",
    "        SummarizeTransformer.from_llm(llm),\n",
    "    ]\n",
    ")\n",
    "vectorstore = ParentVectorStore(\n",
    "    vectorstore=Chroma(\n",
    "        collection_name=\"all_variations_of_chunks\",\n",
    "        embedding_function=embeddings\n",
    "        ),\n",
    "    docstore=docstore,\n",
    "    parent_id_key=\"source\",\n",
    "    id_key=id_key,\n",
    "    parent_transformer=parent_transformer,\n",
    "    child_transformer=child_transformer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f250e7c-cdde-4e05-9f33-bc8f3270b096",
   "metadata": {},
   "source": [
    "## Import documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f40b0257-aab0-4bac-b230-ac3a6130f53e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T07:04:42.986694351Z",
     "start_time": "2023-10-23T07:04:37.538022518Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import WikipediaRetriever\n",
    "\n",
    "documents = WikipediaRetriever(top_k_results=top_k_results).get_relevant_documents(\"mathematic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "82442fca-e199-4652-954b-01aebc8a9583",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T07:05:27.974816204Z",
     "start_time": "2023-10-23T07:04:42.994104099Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pprados/workspace.bda/langchain-parent/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/home/pprados/workspace.bda/langchain-parent/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/home/pprados/workspace.bda/langchain-parent/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/home/pprados/workspace.bda/langchain-parent/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/home/pprados/workspace.bda/langchain-parent/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/home/pprados/workspace.bda/langchain-parent/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/home/pprados/workspace.bda/langchain-parent/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/home/pprados/workspace.bda/langchain-parent/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/home/pprados/workspace.bda/langchain-parent/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/home/pprados/workspace.bda/langchain-parent/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/home/pprados/workspace.bda/langchain-parent/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/home/pprados/workspace.bda/langchain-parent/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/home/pprados/workspace.bda/langchain-parent/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/home/pprados/workspace.bda/langchain-parent/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/home/pprados/workspace.bda/langchain-parent/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/home/pprados/workspace.bda/langchain-parent/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'num_added': 4, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index(\n",
    "    docs_source=documents,\n",
    "    record_manager=record_manager,\n",
    "    vector_store=vectorstore,\n",
    "    cleanup=\"incremental\",\n",
    "    source_id_key=\"source\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c57196-57d4-42fa-ad60-28e59792b340",
   "metadata": {},
   "source": [
    "## Use the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "241da730-38aa-48ec-ba40-d88261b061fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T07:05:34.846821371Z",
     "start_time": "2023-10-23T07:05:34.837645212Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import *\n",
    "from langchain.retrievers.document_compressors import *\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "#TODO: retriever = SelfQueryRetriever. ...\n",
    "retriever= MultiQueryRetriever.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6c929d4-7222-4bd1-882d-a6022722d7e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T07:05:36.041998464Z",
     "start_time": "2023-10-23T07:05:36.021612731Z"
    }
   },
   "outputs": [],
   "source": [
    "similarity_filter = EmbeddingsFilter(\n",
    "        embeddings=embeddings,\n",
    "        similarity_threshold=0.8  # Threshold for determining when two documents are redundant.\n",
    "    )\n",
    "cohere_rerank=CohereRerank(\n",
    "    top_n=top_k_results\n",
    ")\n",
    "# Filter that drops documents that aren't relevant to the query.\n",
    "drop_filter = LLMChainFilter.from_llm(llm)\n",
    "\n",
    "\n",
    "retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=DocumentCompressorPipeline(\n",
    "    transformers=[\n",
    "            similarity_filter,\n",
    "            cohere_rerank,\n",
    "            drop_filter,\n",
    "        ]\n",
    "    ),\n",
    "    base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37669197-ec47-40fb-85c5-1ba0cdb3622e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T07:05:41.836849444Z",
     "start_time": "2023-10-23T07:05:37.018778455Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pprados/workspace.bda/langchain-parent/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/home/pprados/workspace.bda/langchain-parent/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'query': 'names the major mathematical disciplines',\n 'result': '\\n\\nThe major mathematical disciplines include algebra, geometry, calculus, statistics, and trigonometry.'}"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever)\n",
    "qa_chain(\"names the major mathematical disciplines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "756df97c1f537b53"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-parent",
   "language": "python",
   "name": "langchain-parent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
